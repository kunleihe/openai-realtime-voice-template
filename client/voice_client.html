<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Client - Realtime API</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }

        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        .status {
            padding: 10px;
            margin: 10px 0;
            border-radius: 5px;
            font-weight: bold;
        }

        .status.success {
            background-color: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }

        .status.error {
            background-color: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }

        .status.info {
            background-color: #d1ecf1;
            color: #0c5460;
            border: 1px solid #bee5eb;
        }

        .record-button {
            background-color: #28a745;
            color: white;
            border: none;
            padding: 20px 40px;
            font-size: 18px;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s ease;
            margin: 20px 0;
        }

        .record-button:hover {
            background-color: #218838;
            transform: scale(1.05);
        }

        .record-button:disabled {
            background-color: #6c757d;
            cursor: not-allowed;
            transform: none;
        }

        .record-button.recording {
            background-color: #dc3545;
            animation: pulse 1.5s infinite;
        }

        @keyframes pulse {
            0% {
                opacity: 1;
            }

            50% {
                opacity: 0.7;
            }

            100% {
                opacity: 1;
            }
        }

        .audio-controls {
            margin: 20px 0;
        }

        .audio-player {
            width: 100%;
            margin: 10px 0;
        }

        .recording-info {
            margin: 10px 0;
            padding: 10px;
            background-color: #f8f9fa;
            border-radius: 5px;
            display: none;
        }

        .recording-info.active {
            display: block;
        }

        #messages {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            max-height: 300px;
            overflow-y: auto;
            font-family: monospace;
            margin: 20px 0;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>üé§ Voice Client - Realtime API</h1>

        <div id="status" class="status info">
            Connecting to server...
        </div>

        <div class="audio-controls">
            <button id="recordButton" class="record-button">üé§ Hold to Talk</button>
        </div>

        <div id="recordingInfo" class="recording-info">
            <p><strong>Recording:</strong> <span id="recordingTime">00:00</span></p>
            <p><strong>Audio Format:</strong> <span id="audioFormat">-</span></p>
        </div>

        <div class="audio-controls">
            <h3>Last Recording:</h3>
            <audio id="audioPlayer" class="audio-player" controls style="display: none;"></audio>
            <button id="playbackButton" style="display: none;">‚ñ∂Ô∏è Play Last Recording</button>
        </div>

        <h3>Debug Messages:</h3>
        <div id="messages"></div>
    </div>

    <script>
        class VoiceClient {
            constructor() {
                this.mediaRecorder = null;
                this.audioStream = null;
                this.recordedChunks = [];
                this.isRecording = false;
                this.recordingStartTime = null;
                this.recordingTimer = null;
                this.isInitialized = false;

                // WebSocket properties
                this.websocket = null;
                this.isConnected = false;
                this.audioContext = null;

                this.recordButton = document.getElementById('recordButton');
                this.statusDiv = document.getElementById('status');
                this.messagesDiv = document.getElementById('messages');
                this.audioPlayer = document.getElementById('audioPlayer');
                this.playbackButton = document.getElementById('playbackButton');
                this.recordingInfo = document.getElementById('recordingInfo');
                this.recordingTime = document.getElementById('recordingTime');
                this.audioFormat = document.getElementById('audioFormat');

                this.setupEventListeners();
                this.connectWebSocket();
            }

            setupEventListeners() {
                // Mouse events for desktop
                this.recordButton.addEventListener('mousedown', () => this.handleRecordStart());
                this.recordButton.addEventListener('mouseup', () => this.stopRecording());
                this.recordButton.addEventListener('mouseleave', () => this.stopRecording());

                // Touch events for mobile
                this.recordButton.addEventListener('touchstart', (e) => {
                    e.preventDefault();
                    this.handleRecordStart();
                });
                this.recordButton.addEventListener('touchend', (e) => {
                    e.preventDefault();
                    this.stopRecording();
                });

                this.playbackButton.addEventListener('click', () => this.playLastRecording());
            }

            connectWebSocket() {
                try {
                    this.websocket = new WebSocket('ws://localhost:8000/realtime');

                    this.websocket.onopen = () => {
                        this.isConnected = true;
                        this.logMessage('WebSocket connected to realtime API');

                        // Send session configuration
                        this.sendSessionConfig();
                        this.updateStatus('Ready to talk! Hold the button to start.', 'success');
                    };

                    this.websocket.onmessage = (event) => {
                        this.handleWebSocketMessage(event);
                    };

                    this.websocket.onclose = () => {
                        this.isConnected = false;
                        this.logMessage('WebSocket connection closed');
                        this.updateStatus('Connection lost. Refresh page to reconnect.', 'error');
                    };

                    this.websocket.onerror = (error) => {
                        this.logMessage(`WebSocket error: ${error.message || 'Unknown error'}`);
                        this.updateStatus('Connection error. Please try again.', 'error');
                    };

                } catch (error) {
                    this.logMessage(`Failed to connect WebSocket: ${error.message}`);
                    this.updateStatus('Failed to connect to server', 'error');
                }
            }

            sendSessionConfig() {
                const sessionConfig = {
                    type: "session.update",
                    session: {
                        modalities: ["text", "audio"],
                        instructions: "You are a helpful voice assistant. Please respond with both text and audio. Always provide an audio response.",
                        voice: "alloy",
                        input_audio_format: "pcm16",
                        output_audio_format: "pcm16",
                        input_audio_transcription: {
                            model: "whisper-1"
                        },
                        turn_detection: null // Disable server VAD since we're manually controlling
                    }
                };

                this.websocket.send(JSON.stringify(sessionConfig));
                this.logMessage('Sent session configuration');
            }

            handleWebSocketMessage(event) {
                try {
                    const data = JSON.parse(event.data);
                    this.logMessage(`Received: ${data.type}`);

                    switch (data.type) {
                        case 'session.created':
                            this.logMessage('Session created successfully');
                            break;

                        case 'session.updated':
                            this.logMessage('Session updated successfully');
                            break;

                        case 'input_audio_buffer.committed':
                            this.logMessage('Audio buffer committed');
                            break;

                        case 'response.created':
                            this.logMessage('Response creation started');
                            break;

                        case 'response.output_item.added':
                            this.logMessage('Response item added');
                            break;

                        case 'response.content_part.added':
                            this.logMessage('Response content part added');
                            break;

                        case 'response.audio.delta':
                            this.handleAudioDelta(data.delta);
                            break;

                        case 'response.audio.done':
                            this.handleAudioComplete();
                            this.playCompleteResponse();
                            break;

                        case 'response.done':
                            this.logMessage('Response completed');
                            // Check if we got any audio response
                            if (!this.responseAudioBuffer || this.responseAudioBuffer.length === 0) {
                                this.logMessage('No audio response received - trying to trigger audio generation');
                                this.updateStatus('No audio response - ready to try again', 'error');
                            } else {
                                this.playCompleteResponse();
                            }
                            break;

                        case 'conversation.item.created':
                            this.logMessage('Conversation item created');
                            break;

                        case 'response.audio_transcript.delta':
                            // Handle transcript if needed
                            if (data.delta) {
                                this.logMessage(`Transcript: ${data.delta}`);
                            }
                            break;

                        case 'response.audio_transcript.done':
                            this.logMessage('Audio transcript completed');
                            break;

                        case 'response.content_part.done':
                            this.logMessage('Response content part completed');
                            break;

                        case 'response.output_item.done':
                            this.logMessage('Response output item completed');
                            break;

                        case 'rate_limits.updated':
                            this.logMessage('Rate limits updated');
                            break;

                        case 'error':
                            this.logMessage(`API Error: ${data.error.message}`);
                            this.updateStatus(`Error: ${data.error.message}`, 'error');
                            break;

                        default:
                            this.logMessage(`Unhandled message type: ${data.type}`);
                    }

                } catch (e) {
                    this.logMessage(`Error parsing message: ${e.message}`);
                }
            }

            handleAudioDelta(audioBase64) {
                try {
                    if (!this.responseAudioBuffer) {
                        this.responseAudioBuffer = [];
                    }

                    // Decode base64 audio data (PCM16)
                    const audioData = atob(audioBase64);
                    const audioBytes = new Uint8Array(audioData.length);
                    for (let i = 0; i < audioData.length; i++) {
                        audioBytes[i] = audioData.charCodeAt(i);
                    }

                    // Convert PCM16 bytes to Int16Array
                    const pcm16Data = new Int16Array(audioBytes.buffer);

                    // Add to response buffer (accumulate all chunks)
                    this.responseAudioBuffer.push(pcm16Data);

                    this.logMessage(`Received audio delta: ${pcm16Data.length} samples`);

                    // Don't play yet - wait for complete response

                } catch (error) {
                    this.logMessage(`Error handling audio delta: ${error.message}`);
                }
            }

            async startAudioPlayback() {
                if (this.isPlayingResponse || !this.responseAudioBuffer || this.responseAudioBuffer.length === 0) return;

                this.isPlayingResponse = true;
                this.updateStatus('Playing response...', 'info');
                this.logMessage('Started audio response playback');

                try {
                    // Concatenate all PCM16 chunks
                    let totalLength = 0;
                    for (const chunk of this.responseAudioBuffer) {
                        totalLength += chunk.length;
                    }

                    const combinedPCM16 = new Int16Array(totalLength);
                    let offset = 0;
                    for (const chunk of this.responseAudioBuffer) {
                        combinedPCM16.set(chunk, offset);
                        offset += chunk.length;
                    }

                    // Convert PCM16 to AudioBuffer for playback
                    const audioBuffer = this.audioContext.createBuffer(1, combinedPCM16.length, 24000);
                    const channelData = audioBuffer.getChannelData(0);

                    // Convert Int16 to Float32 for AudioBuffer
                    for (let i = 0; i < combinedPCM16.length; i++) {
                        channelData[i] = combinedPCM16[i] / 32768.0;
                    }

                    // Create and play audio source
                    const source = this.audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(this.audioContext.destination);

                    source.onended = () => {
                        this.isPlayingResponse = false;
                        this.updateStatus('Ready to talk! Hold the button to start.', 'success');
                        this.logMessage('Audio response playback completed');
                    };

                    source.start(0);
                    this.logMessage(`Playing ${combinedPCM16.length} audio samples`);

                } catch (error) {
                    this.logMessage(`Audio playback error: ${error.message}`);
                    this.isPlayingResponse = false;
                    this.updateStatus('Ready to talk! Hold the button to start.', 'success');
                }
            }

            handleAudioComplete() {
                this.logMessage('Audio response complete - ready to play');
            }

            playCompleteResponse() {
                if (!this.responseAudioBuffer || this.responseAudioBuffer.length === 0) {
                    this.logMessage('No audio response to play');
                    this.updateStatus('Ready to talk! Hold the button to start.', 'success');
                    return;
                }

                try {
                    this.updateStatus('Playing AI response...', 'info');

                    // Combine all response audio chunks
                    let totalLength = 0;
                    for (const chunk of this.responseAudioBuffer) {
                        totalLength += chunk.length;
                    }

                    const combinedPCM16 = new Int16Array(totalLength);
                    let offset = 0;
                    for (const chunk of this.responseAudioBuffer) {
                        combinedPCM16.set(chunk, offset);
                        offset += chunk.length;
                    }

                    this.logMessage(`Playing complete AI response: ${combinedPCM16.length} samples (${(combinedPCM16.length / 24000).toFixed(2)}s)`);

                    // Create WAV file for playback
                    const wavBuffer = this.createWavFile(combinedPCM16, 24000);
                    const wavBlob = new Blob([wavBuffer], { type: 'audio/wav' });
                    const wavUrl = URL.createObjectURL(wavBlob);

                    // Play using HTML5 audio
                    const audio = new Audio(wavUrl);
                    audio.play();

                    audio.onended = () => {
                        this.logMessage('AI response playback completed');
                        this.updateStatus('Ready to talk! Hold the button to start.', 'success');
                        URL.revokeObjectURL(wavUrl);

                        // Complete cleanup for next conversation
                        this.responseAudioBuffer = [];
                        this.accumulatedAudioData = [];
                        this.clearAudioBuffer();
                    };

                    audio.onerror = (error) => {
                        this.logMessage(`Audio playback error: ${error.message || 'Unknown error'}`);
                        this.updateStatus('Playback error - Ready to talk again', 'error');
                        URL.revokeObjectURL(wavUrl);

                        // Complete cleanup for next conversation
                        this.responseAudioBuffer = [];
                        this.accumulatedAudioData = [];
                        this.clearAudioBuffer();
                    };

                } catch (error) {
                    this.logMessage(`Error playing response: ${error.message}`);
                    this.updateStatus('Playback error - Ready to talk again', 'error');

                    // Complete cleanup for next conversation
                    this.responseAudioBuffer = [];
                    this.accumulatedAudioData = [];
                    this.clearAudioBuffer();
                }
            }

            createWavFile(pcm16Data, sampleRate) {
                const length = pcm16Data.length;
                const buffer = new ArrayBuffer(44 + length * 2);
                const view = new DataView(buffer);

                // WAV header
                const writeString = (offset, string) => {
                    for (let i = 0; i < string.length; i++) {
                        view.setUint8(offset + i, string.charCodeAt(i));
                    }
                };

                writeString(0, 'RIFF');
                view.setUint32(4, 36 + length * 2, true);
                writeString(8, 'WAVE');
                writeString(12, 'fmt ');
                view.setUint32(16, 16, true);
                view.setUint16(20, 1, true);
                view.setUint16(22, 1, true);
                view.setUint32(24, sampleRate, true);
                view.setUint32(28, sampleRate * 2, true);
                view.setUint16(32, 2, true);
                view.setUint16(34, 16, true);
                writeString(36, 'data');
                view.setUint32(40, length * 2, true);

                // PCM data
                let offset = 44;
                for (let i = 0; i < length; i++) {
                    view.setInt16(offset, pcm16Data[i], true);
                    offset += 2;
                }

                return buffer;
            }

            async handleRecordStart() {
                if (!this.isConnected) {
                    this.updateStatus('Not connected to server', 'error');
                    return;
                }

                if (!this.isInitialized) {
                    await this.initializeMicrophone();
                }
                if (this.isInitialized) {
                    this.startRecording();
                }
            }

            async initializeMicrophone() {
                try {
                    this.updateStatus('Requesting microphone access...', 'info');

                    // Request microphone access
                    this.audioStream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 24000, // Optimal for OpenAI Realtime API
                            channelCount: 1 // Mono audio
                        }
                    });

                    // Initialize AudioContext for audio processing
                    this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
                        sampleRate: 24000
                    });

                    this.logMessage('Microphone access granted');
                    this.logMessage(`Audio track settings: ${JSON.stringify(this.audioStream.getAudioTracks()[0].getSettings())}`);

                    // Create audio processing pipeline for PCM16 conversion
                    this.setupAudioProcessing();

                    // Setup MediaRecorder for local recording (fallback)
                    const options = {
                        mimeType: this.getSupportedMimeType(),
                        audioBitsPerSecond: 24000
                    };

                    this.mediaRecorder = new MediaRecorder(this.audioStream, options);
                    this.audioFormat.textContent = "PCM16 24kHz (converted from " + options.mimeType + ")";

                    this.mediaRecorder.ondataavailable = (event) => {
                        if (event.data.size > 0) {
                            this.recordedChunks.push(event.data);
                            this.logMessage(`Audio chunk received: ${event.data.size} bytes`);
                        }
                    };

                    // Handle when recording stops
                    this.mediaRecorder.onstop = () => {
                        this.processRecordedAudio();
                    };



                    this.updateStatus('Microphone ready! Hold the button to record.', 'success');
                    this.isInitialized = true;

                } catch (error) {
                    this.logMessage(`Error initializing microphone: ${error.message}`);
                    this.updateStatus(`Microphone access failed: ${error.message}`, 'error');

                    if (error.name === 'NotAllowedError') {
                        this.updateStatus('Microphone access denied. Please allow microphone permissions and refresh the page.', 'error');
                    }
                }
            }

            getSupportedMimeType() {
                const types = [
                    'audio/webm;codecs=opus',
                    'audio/webm',
                    'audio/mp4',
                    'audio/wav'
                ];

                for (const type of types) {
                    if (MediaRecorder.isTypeSupported(type)) {
                        this.logMessage(`Using MIME type: ${type}`);
                        return type;
                    }
                }

                this.logMessage('No preferred MIME type supported, using default');
                return '';
            }

            setupAudioProcessing() {
                // Create audio processing nodes for recording
                this.source = this.audioContext.createMediaStreamSource(this.audioStream);
                this.processor = this.audioContext.createScriptProcessor(1024, 1, 1);

                // Accumulate audio data while recording (don't send real-time)
                this.accumulatedAudioData = [];

                this.processor.onaudioprocess = (event) => {
                    if (this.isRecording) {
                        const inputBuffer = event.inputBuffer;
                        const inputData = inputBuffer.getChannelData(0);

                        // Convert float32 to PCM16 and accumulate
                        const pcm16Data = this.floatToPCM16(inputData);
                        this.accumulatedAudioData.push(pcm16Data);
                    }
                };

                // Connect the nodes but DON'T connect to destination to prevent echo
                this.source.connect(this.processor);
                // Removed: this.processor.connect(this.audioContext.destination);
            }

            floatToPCM16(float32Array) {
                const pcm16Array = new Int16Array(float32Array.length);
                for (let i = 0; i < float32Array.length; i++) {
                    // Convert float32 (-1 to 1) to int16 (-32768 to 32767)
                    const sample = Math.max(-1, Math.min(1, float32Array[i]));
                    pcm16Array[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                }
                return pcm16Array;
            }

            async processRecordedAudio() {
                if (!this.recordedChunks || this.recordedChunks.length === 0) {
                    this.logMessage('No recorded audio chunks to process');
                    this.updateStatus('No audio recorded', 'error');
                    return;
                }

                try {
                    this.logMessage(`Processing ${this.recordedChunks.length} audio chunks`);

                    // Combine all recorded chunks into a single blob
                    const combinedBlob = new Blob(this.recordedChunks, { type: 'audio/webm' });
                    this.logMessage(`Combined audio blob: ${combinedBlob.size} bytes`);

                    // Convert to PCM16 for OpenAI API
                    const pcm16Data = await this.convertWebMToPCM16(combinedBlob);

                    if (pcm16Data && pcm16Data.length > 0) {
                        this.logMessage(`Converted to PCM16: ${pcm16Data.length} samples (${(pcm16Data.length / 24000).toFixed(2)}s)`);

                        // Log first few and last few samples for debugging
                        const firstSamples = Array.from(pcm16Data.slice(0, 10));
                        const lastSamples = Array.from(pcm16Data.slice(-10));
                        this.logMessage(`Audio fingerprint - First: [${firstSamples.join(',')}] Last: [${lastSamples.join(',')}]`);

                        // Send to API
                        this.sendAudioToAPI(pcm16Data);
                    } else {
                        this.logMessage('Failed to convert audio to PCM16');
                        this.updateStatus('Audio conversion failed', 'error');
                    }

                } catch (error) {
                    this.logMessage(`Error processing recorded audio: ${error.message}`);
                    this.updateStatus('Error processing audio', 'error');
                }
            }

            async convertWebMToPCM16(audioBlob) {
                try {
                    this.logMessage(`Starting audio conversion - blob size: ${audioBlob.size} bytes`);

                    // Create audio context for decoding
                    const arrayBuffer = await audioBlob.arrayBuffer();
                    this.logMessage(`ArrayBuffer created: ${arrayBuffer.byteLength} bytes`);

                    const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
                    this.logMessage(`Audio decoded - duration: ${audioBuffer.duration.toFixed(2)}s, channels: ${audioBuffer.numberOfChannels}, sampleRate: ${audioBuffer.sampleRate}`);

                    // Get audio data (mono channel)
                    const channelData = audioBuffer.getChannelData(0);
                    this.logMessage(`Channel data length: ${channelData.length} samples`);

                    // Check if audio has actual content (not silence)
                    let maxAmplitude = 0;
                    for (let i = 0; i < channelData.length; i++) {
                        maxAmplitude = Math.max(maxAmplitude, Math.abs(channelData[i]));
                    }
                    this.logMessage(`Audio max amplitude: ${maxAmplitude.toFixed(4)} (should be > 0.001 for actual speech)`);

                    if (maxAmplitude < 0.001) {
                        this.logMessage('WARNING: Audio appears to be silent or very quiet');
                    }

                    // Resample to 24kHz if needed
                    const targetSampleRate = 24000;
                    const sourceSampleRate = audioBuffer.sampleRate;

                    let resampledData;
                    if (sourceSampleRate !== targetSampleRate) {
                        resampledData = this.resample(channelData, sourceSampleRate, targetSampleRate);
                        this.logMessage(`Resampled from ${sourceSampleRate}Hz to ${targetSampleRate}Hz - output length: ${resampledData.length}`);
                    } else {
                        resampledData = channelData;
                        this.logMessage(`No resampling needed (already ${targetSampleRate}Hz)`);
                    }

                    // Convert to PCM16
                    const pcm16Data = this.floatToPCM16(resampledData);
                    this.logMessage(`PCM16 conversion complete - ${pcm16Data.length} samples`);

                    return pcm16Data;

                } catch (error) {
                    this.logMessage(`ERROR converting WebM to PCM16: ${error.message}`);
                    this.logMessage(`Error stack: ${error.stack}`);
                    return null;
                }
            }

            resample(inputData, sourceSampleRate, targetSampleRate) {
                const ratio = sourceSampleRate / targetSampleRate;
                const outputLength = Math.floor(inputData.length / ratio);
                const outputData = new Float32Array(outputLength);

                for (let i = 0; i < outputLength; i++) {
                    const sourceIndex = i * ratio;
                    const index = Math.floor(sourceIndex);
                    const fraction = sourceIndex - index;

                    if (index + 1 < inputData.length) {
                        outputData[i] = inputData[index] * (1 - fraction) + inputData[index + 1] * fraction;
                    } else {
                        outputData[i] = inputData[index];
                    }
                }

                return outputData;
            }

            sendAudioToAPI(pcm16Data) {
                if (this.websocket && this.isConnected && this.websocket.readyState === WebSocket.OPEN) {
                    try {
                        this.logMessage(`Starting audio transmission - ${pcm16Data.length} samples`);
                        this.updateStatus('Sending audio to AI...', 'info');

                        // Convert PCM16 to base64
                        const uint8Array = new Uint8Array(pcm16Data.buffer);
                        let binary = '';
                        for (let i = 0; i < uint8Array.length; i++) {
                            binary += String.fromCharCode(uint8Array[i]);
                        }
                        const audioBase64 = btoa(binary);

                        this.logMessage(`Audio converted to base64 - ${audioBase64.length} characters`);

                        // First clear any existing audio buffer
                        this.clearAudioBuffer();

                        // Wait a bit, then send audio, then create response
                        setTimeout(() => {
                            // Send as input_audio_buffer.append message
                            const appendMessage = {
                                type: "input_audio_buffer.append",
                                audio: audioBase64
                            };

                            this.websocket.send(JSON.stringify(appendMessage));
                            this.logMessage(`‚úì Audio sent to API: ${pcm16Data.length * 2} bytes`);

                            // Wait a bit then commit the buffer
                            setTimeout(() => {
                                this.commitAudioBuffer();
                                this.logMessage(`‚úì Audio buffer committed`);

                                // Wait a bit more then create response
                                setTimeout(() => {
                                    this.createResponse();
                                    this.logMessage(`‚úì Response creation requested`);
                                }, 100);
                            }, 100);
                        }, 200);

                    } catch (error) {
                        this.logMessage(`ERROR sending audio to API: ${error.message}`);
                        this.updateStatus('Error sending audio', 'error');
                    }
                } else {
                    this.logMessage('ERROR: WebSocket not ready');
                    this.updateStatus('Connection error', 'error');
                }
            }

            startRecording() {
                if (!this.mediaRecorder || this.isRecording) return;

                try {
                    // Clear all audio buffers for fresh recording
                    this.recordedChunks = [];
                    this.accumulatedAudioData = [];
                    this.responseAudioBuffer = [];

                    // Clear server-side audio buffer only
                    this.clearAudioBuffer();

                    this.mediaRecorder.start(100); // Collect data for local recording
                    this.isRecording = true;
                    this.recordingStartTime = Date.now();

                    this.recordButton.textContent = 'üî¥ Recording...';
                    this.recordButton.classList.add('recording');
                    this.recordingInfo.classList.add('active');
                    this.updateStatus('Recording audio...', 'info');

                    this.startRecordingTimer();
                    this.logMessage('Recording started - ready to capture your voice');

                } catch (error) {
                    this.logMessage(`Error starting recording: ${error.message}`);
                    this.updateStatus(`Recording failed: ${error.message}`, 'error');
                }
            }

            createResponse() {
                if (this.websocket && this.isConnected && this.websocket.readyState === WebSocket.OPEN) {
                    const message = {
                        type: "response.create",
                        response: {
                            modalities: ["text", "audio"],
                            instructions: "Please provide an audio response. Respond naturally and conversationally.",
                            voice: "alloy",
                            output_audio_format: "pcm16"
                        }
                    };

                    this.websocket.send(JSON.stringify(message));
                    this.logMessage('Created response request with audio output');
                }
            }

            createNewConversation() {
                if (this.websocket && this.isConnected && this.websocket.readyState === WebSocket.OPEN) {
                    // First clear the audio buffer
                    const clearMessage = {
                        type: "input_audio_buffer.clear"
                    };
                    this.websocket.send(JSON.stringify(clearMessage));

                    // Then create a new conversation item to reset context
                    const conversationMessage = {
                        type: "conversation.item.create",
                        item: {
                            id: `user_${Date.now()}`,
                            type: "message",
                            role: "user",
                            content: [
                                {
                                    type: "input_text",
                                    text: "Starting new conversation"
                                }
                            ]
                        }
                    };
                    this.websocket.send(JSON.stringify(conversationMessage));

                    this.logMessage('Created new conversation - fresh context');
                }
            }

            clearAudioBuffer() {
                if (this.websocket && this.isConnected && this.websocket.readyState === WebSocket.OPEN) {
                    const message = {
                        type: "input_audio_buffer.clear"
                    };

                    this.websocket.send(JSON.stringify(message));
                    this.logMessage('Cleared audio buffer');
                }
            }

            commitAudioBuffer() {
                if (this.websocket && this.isConnected && this.websocket.readyState === WebSocket.OPEN) {
                    const message = {
                        type: "input_audio_buffer.commit"
                    };

                    this.websocket.send(JSON.stringify(message));
                    this.logMessage('Committed audio buffer');
                }
            }

            stopRecording() {
                if (!this.mediaRecorder || !this.isRecording) return;

                try {
                    this.mediaRecorder.stop();
                    this.isRecording = false;

                    this.recordButton.textContent = 'üé§ Hold to Talk';
                    this.recordButton.classList.remove('recording');
                    this.recordingInfo.classList.remove('active');
                    this.stopRecordingTimer();

                    this.logMessage('Recording stopped - processing audio');
                    this.updateStatus('Processing... converting audio format', 'info');

                    // The MediaRecorder's onstop event will trigger processRecordedAudio()

                } catch (error) {
                    this.logMessage(`Error stopping recording: ${error.message}`);
                }
            }

            startRecordingTimer() {
                this.recordingTimer = setInterval(() => {
                    const elapsed = Date.now() - this.recordingStartTime;
                    const seconds = Math.floor(elapsed / 1000);
                    const minutes = Math.floor(seconds / 60);
                    const displaySeconds = seconds % 60;
                    this.recordingTime.textContent =
                        `${minutes.toString().padStart(2, '0')}:${displaySeconds.toString().padStart(2, '0')}`;
                }, 1000);
            }

            stopRecordingTimer() {
                if (this.recordingTimer) {
                    clearInterval(this.recordingTimer);
                    this.recordingTimer = null;
                }
                this.recordingTime.textContent = '00:00';
            }

            handleRecordingComplete() {
                if (this.recordedChunks.length === 0) {
                    this.updateStatus('No audio data recorded', 'error');
                    return;
                }

                const audioBlob = new Blob(this.recordedChunks, {
                    type: this.mediaRecorder.mimeType
                });

                const audioUrl = URL.createObjectURL(audioBlob);
                this.audioPlayer.src = audioUrl;
                this.audioPlayer.style.display = 'block';
                this.playbackButton.style.display = 'block';

                const duration = (Date.now() - this.recordingStartTime) / 1000;
                this.logMessage(`Recording complete: ${audioBlob.size} bytes, ${duration.toFixed(2)}s duration`);
                this.updateStatus(`Recording saved! Duration: ${duration.toFixed(2)}s, Size: ${audioBlob.size} bytes`, 'success');

                // This is where we'll later add WebSocket transmission
                this.logMessage('Ready to transmit audio data to WebSocket (not implemented yet)');
            }

            playLastRecording() {
                if (this.audioPlayer.src) {
                    this.audioPlayer.play();
                    this.logMessage('Playing back recorded audio');
                }
            }

            updateStatus(message, type = 'info') {
                this.statusDiv.textContent = message;
                this.statusDiv.className = `status ${type}`;
            }

            logMessage(message) {
                const timestamp = new Date().toLocaleTimeString();
                this.messagesDiv.innerHTML += `[${timestamp}] ${message}\n`;
                this.messagesDiv.scrollTop = this.messagesDiv.scrollHeight;
            }
        }

        // Initialize the voice client when the page loads
        document.addEventListener('DOMContentLoaded', () => {
            const client = new VoiceClient();
            window.voiceClient = client; // Make it globally accessible for debugging
        });
    </script>
</body>

</html>